{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3176,"sourceType":"datasetVersion","datasetId":1835}],"dockerImageVersionId":29661,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Prepare Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:31.138204Z","iopub.execute_input":"2026-02-16T17:13:31.138501Z","iopub.status.idle":"2026-02-16T17:13:31.146119Z","shell.execute_reply.started":"2026-02-16T17:13:31.138459Z","shell.execute_reply":"2026-02-16T17:13:31.144836Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"with open(\"../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\") as file:\n    data = file.readlines()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:31.148404Z","iopub.execute_input":"2026-02-16T17:13:31.148758Z","iopub.status.idle":"2026-02-16T17:13:31.425067Z","shell.execute_reply.started":"2026-02-16T17:13:31.148691Z","shell.execute_reply":"2026-02-16T17:13:31.424218Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"len(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:31.426208Z","iopub.execute_input":"2026-02-16T17:13:31.426651Z","iopub.status.idle":"2026-02-16T17:13:31.432017Z","shell.execute_reply.started":"2026-02-16T17:13:31.426602Z","shell.execute_reply":"2026-02-16T17:13:31.430861Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"400000"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"for i in range(len(data)):\n    data[i] = data[i][:-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:31.433259Z","iopub.execute_input":"2026-02-16T17:13:31.433548Z","iopub.status.idle":"2026-02-16T17:13:31.546332Z","shell.execute_reply.started":"2026-02-16T17:13:31.433501Z","shell.execute_reply":"2026-02-16T17:13:31.545385Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"data_dict = dict()\n\nfor i in range(len(data)):\n    split_data = data[i].split()\n    data_dict[split_data[0]] = np.array(split_data[1:]).astype('float64')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:31.548705Z","iopub.execute_input":"2026-02-16T17:13:31.548961Z","iopub.status.idle":"2026-02-16T17:13:41.374167Z","shell.execute_reply.started":"2026-02-16T17:13:31.548914Z","shell.execute_reply":"2026-02-16T17:13:41.373081Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"data_dict[\"the\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:41.375885Z","iopub.execute_input":"2026-02-16T17:13:41.376126Z","iopub.status.idle":"2026-02-16T17:13:41.381533Z","shell.execute_reply.started":"2026-02-16T17:13:41.376081Z","shell.execute_reply":"2026-02-16T17:13:41.380815Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"### Cosine Similarity Example","metadata":{}},{"cell_type":"code","source":"def cosine_similarity(a, b):\n    nominator = np.dot(a, b)\n    \n    a_norm = np.sqrt(np.sum(a**2))\n    b_norm = np.sqrt(np.sum(b**2))\n    \n    denominator = a_norm * b_norm\n    \n    cosine_similarity = nominator / denominator\n    \n    return cosine_similarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:41.382457Z","iopub.execute_input":"2026-02-16T17:13:41.382613Z","iopub.status.idle":"2026-02-16T17:13:41.391713Z","shell.execute_reply.started":"2026-02-16T17:13:41.382585Z","shell.execute_reply":"2026-02-16T17:13:41.390889Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"table = data_dict[\"table\"]\ndesk = data_dict[\"desk\"]\nfootball = data_dict[\"football\"]\nbaseball = data_dict[\"baseball\"]\nwater = data_dict[\"water\"]\nfire = data_dict[\"fire\"]\ncomputer = data_dict[\"computer\"]\ncalculator = data_dict[\"calculator\"]\nnumber = data_dict[\"number\"]\nmath = data_dict[\"math\"]\nboy = data_dict[\"boy\"]\ngirl = data_dict[\"girl\"]\nsad = data_dict[\"sad\"]\nhappy = data_dict[\"happy\"]\ngood = data_dict[\"good\"]\nbad = data_dict[\"bad\"]\nturkey = data_dict[\"turkey\"]\ntelevision = data_dict[\"television\"]\nawesome = data_dict[\"awesome\"]\ngreat = data_dict[\"great\"]\ncoffee = data_dict[\"coffee\"]\ngiraffe = data_dict[\"giraffe\"]\ncat = data_dict[\"cat\"]\nbarcelona = data_dict[\"barcelona\"]\nschool = data_dict[\"school\"]\ndisaster = data_dict[\"disaster\"]\n\nprint(f\"Cosine similarity for pair (table, desk) = {cosine_similarity(table, desk)}\")\nprint(f\"Cosine similarity for pair (football, baseball) = {cosine_similarity(football, baseball)}\")\nprint(f\"Cosine similarity for pair (water, fire) = {cosine_similarity(water, fire)}\")\nprint(f\"Cosine similarity for pair (computer, calculator) = {cosine_similarity(computer, calculator)}\")\nprint(f\"Cosine similarity for pair (number, math) = {cosine_similarity(number, math)}\")\nprint(f\"Cosine similarity for pair (boy, girl) = {cosine_similarity(boy, girl)}\")\nprint(f\"Cosine similarity for pair (sad, happy) = {cosine_similarity(sad, happy)}\")\nprint(f\"Cosine similarity for pair (good, bad) = {cosine_similarity(good, bad)}\")\nprint(f\"Cosine similarity for pair (turkey, television) = {cosine_similarity(turkey, television)}\")\nprint(f\"Cosine similarity for pair (awesome, great) = {cosine_similarity(awesome, great)}\")\nprint(f\"Cosine similarity for pair (coffee, giraffe) = {cosine_similarity(coffee, giraffe)}\")\nprint(f\"Cosine similarity for pair (cat, barcelona) = {cosine_similarity(cat, barcelona)}\")\nprint(f\"Cosine similarity for pair (school, disaster) = {cosine_similarity(school, disaster)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:41.392620Z","iopub.execute_input":"2026-02-16T17:13:41.392802Z","iopub.status.idle":"2026-02-16T17:13:41.417943Z","shell.execute_reply.started":"2026-02-16T17:13:41.392769Z","shell.execute_reply":"2026-02-16T17:13:41.416941Z"}},"outputs":[{"name":"stdout","text":"Cosine similarity for pair (table, desk) = 0.56312532465622\nCosine similarity for pair (football, baseball) = 0.7990507471765448\nCosine similarity for pair (water, fire) = 0.6159761736263326\nCosine similarity for pair (computer, calculator) = 0.5805204352195886\nCosine similarity for pair (number, math) = 0.3923536921031839\nCosine similarity for pair (boy, girl) = 0.9327198629646993\nCosine similarity for pair (sad, happy) = 0.689063223084822\nCosine similarity for pair (good, bad) = 0.7964893661716318\nCosine similarity for pair (turkey, television) = 0.3478390727581068\nCosine similarity for pair (awesome, great) = 0.54452994054594\nCosine similarity for pair (coffee, giraffe) = 0.039573626896088\nCosine similarity for pair (cat, barcelona) = 0.02882096607257644\nCosine similarity for pair (school, disaster) = 0.2852025050456493\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def euclidean_distance(a, b):\n    \"\"\"\n    Calculates the Euclidean distance between two vectors.\n    Formula: ||A - B||\n    \"\"\"\n    a = np.array(a, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    return np.linalg.norm(a - b)\n\nprint(f\"Euclidean Distances:\")\nprint(f\"(table, desk) = {euclidean_distance(table, desk)}\")\nprint(f\"(football, baseball) = {euclidean_distance(football, baseball)}\")\nprint(f\"(water, fire) = {euclidean_distance(water, fire)}\")\nprint(f\"(computer, calculator) = {euclidean_distance(computer, calculator)}\")\nprint(f\"(number, math) = {euclidean_distance(number, math)}\")\nprint(f\"(boy, girl) = {euclidean_distance(boy, girl)}\")\nprint(f\"(sad, happy) = {euclidean_distance(sad, happy)}\")\nprint(f\"(good, bad) = {euclidean_distance(good, bad)}\")\nprint(f\"(turkey, television) = {euclidean_distance(turkey, television)}\")\nprint(f\"(awesome, great) = {euclidean_distance(awesome, great)}\")\nprint(f\"(coffee, giraffe) = {euclidean_distance(coffee, giraffe)}\")\nprint(f\"(cat, barcelona) = {euclidean_distance(cat, barcelona)}\")\nprint(f\"(school, disaster) = {euclidean_distance(school, disaster)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:41.419099Z","iopub.execute_input":"2026-02-16T17:13:41.419325Z","iopub.status.idle":"2026-02-16T17:13:41.428793Z","shell.execute_reply.started":"2026-02-16T17:13:41.419285Z","shell.execute_reply":"2026-02-16T17:13:41.427492Z"}},"outputs":[{"name":"stdout","text":"Euclidean Distances:\n(table, desk) = 4.704135012081877\n(football, baseball) = 3.718578193888761\n(water, fire) = 4.917517613751779\n(computer, calculator) = 5.005315848012591\n(number, math) = 6.120560145939014\n(boy, girl) = 2.0426333096686737\n(sad, happy) = 3.8399498989360525\n(good, bad) = 3.3188904070491088\n(turkey, television) = 6.488383561488301\n(awesome, great) = 4.578660135608451\n(coffee, giraffe) = 6.440001424040119\n(cat, barcelona) = 6.833869604598847\n(school, disaster) = 6.54555871572576\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### Three Pairs with highest Cosine Similarity\n(football,baseball) =0.799\\\n(boy,girl) = 0.932\\\n(good,bad) = 0.796\n\nYes, Football and Baseball shares same semantics as sports equipment and both are ball. Boy ang girl share same age group, semnatically. Good and Bad are outcomes so these make sense.\n\n### Look at the pair (good, bad). They are opposites, yet they have a high cosine similarity score (approx 0.8). Why do you think vector embeddings place them close together?\n\nThey represent output or adjective thats why they are close together, also if we go with logic for calculating such vectors, whenever we use good we can replace with bad hence surrounding words remain similar and hence high similarity score.","metadata":{}},{"cell_type":"markdown","source":"### Word Analogies","metadata":{}},{"cell_type":"code","source":"def find_word(a, b, c, data_dict):\n    a, b, c = a.lower(), b.lower(), c.lower()\n    a_vector, b_vector, c_vector = data_dict[a], data_dict[b], data_dict[c]\n    \n    all_words = data_dict.keys()\n    max_cosine_similarity = -1000\n    best_match_word = None\n    \n    for word in all_words:\n        if word in [a, b, c]:\n            continue\n            \n        cos_sim = cosine_similarity(np.subtract(b_vector, a_vector), np.subtract(data_dict[word], c_vector))\n        \n        if cos_sim > max_cosine_similarity:\n            max_cosine_similarity = cos_sim\n            best_match_word = word\n            \n    return best_match_word, cos_sim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:41.429715Z","iopub.execute_input":"2026-02-16T17:13:41.429868Z","iopub.status.idle":"2026-02-16T17:13:41.442855Z","shell.execute_reply.started":"2026-02-16T17:13:41.429840Z","shell.execute_reply":"2026-02-16T17:13:41.442118Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def find_word_euclidean(a, b, c, data_dict):\n    a, b, c = a.lower(), b.lower(), c.lower()\n    a_vector, b_vector, c_vector = data_dict[a], data_dict[b], data_dict[c]\n    \n    all_words = data_dict.keys()\n    \n    min_distance = float('inf') \n    best_match_word = None\n    \n    # Calculate target vector once (b - a)\n    target_relationship = np.subtract(b_vector, a_vector)\n    \n    for word in all_words:\n        if word in [a, b, c]:\n            continue\n        \n        # Calculate candidate relationship (d - c)\n        candidate_relationship = np.subtract(data_dict[word], c_vector)\n        \n        # Calculate distance between the two relationship vectors\n        dist = euclidean_distance(target_relationship, candidate_relationship)\n        \n        # FIX 2: Look for the SMALLEST distance (minimize error)\n        if dist < min_distance:\n            min_distance = dist\n            best_match_word = word\n            \n    return best_match_word, min_distance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:41.443867Z","iopub.execute_input":"2026-02-16T17:13:41.444060Z","iopub.status.idle":"2026-02-16T17:13:41.456368Z","shell.execute_reply.started":"2026-02-16T17:13:41.444021Z","shell.execute_reply":"2026-02-16T17:13:41.455671Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"words_bag = [\n    ('boy', 'girl', 'man'),\n    ('bat', 'baseball', 'ball'),\n    ('book', 'library', 'coffee'),\n    ('orange', 'juice', 'apple'),\n    ('turkey', 'turkish', 'colombia')\n]\n\nfor words in words_bag:\n    d, cos_sim = find_word(*words, data_dict)\n    print(\"({}, {}) ----> ({}, {}) with {} difference\".format(*words, d, cos_sim))\n\n    d, cos_sim = find_word_euclidean(*words, data_dict)\n    print(\"({}, {}) ----> ({}, {}) with {} difference\".format(*words, d, cos_sim))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:13:41.457413Z","iopub.execute_input":"2026-02-16T17:13:41.457636Z","iopub.status.idle":"2026-02-16T17:14:16.662213Z","shell.execute_reply.started":"2026-02-16T17:13:41.457596Z","shell.execute_reply":"2026-02-16T17:14:16.661284Z"}},"outputs":[{"name":"stdout","text":"(boy, girl) ----> (man, woman) with -0.0340757677824383 difference\n(boy, girl) ----> (man, woman) with 1.956399832275647 difference\n(bat, baseball) ----> (ball, basketball) with 0.09564220586831747 difference\n(bat, baseball) ----> (ball, basketball) with 4.251404585612963 difference\n(book, library) ----> (coffee, heliospheric) with 0.10581179044448522 difference\n(book, library) ----> (coffee, warehouse) with 4.838743250989605 difference\n(orange, juice) ----> (apple, juices) with -0.2351728294582547 difference\n(orange, juice) ----> (apple, processor) with 5.36831789214182 difference\n(turkey, turkish) ----> (colombia, colombian) with 0.1700399408595448 difference\n(turkey, turkish) ----> (colombia, colombian) with 2.3834937989013105 difference\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def get_nearest_neighbors(target_word, data_dict, top_n=5):\n    \"\"\"\n    Finds the top_n words with the highest cosine similarity to the target_word.\n    \"\"\"\n    target_word = target_word.lower()\n    \n    # Check if word exists in the dictionary\n    if target_word not in data_dict:\n        return f\"'{target_word}' not found in the vocabulary.\"\n    \n    target_vector = data_dict[target_word]\n    similarities = []\n    \n    # Calculate cosine similarity with every other word\n    for word, vector in data_dict.items():\n        if word == target_word:\n            continue\n            \n        sim = cosine_similarity(target_vector, vector)\n        similarities.append((word, sim))\n    \n    # Sort by similarity in descending order (highest first)\n    # x[1] refers to the similarity score\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    \n    return similarities[:top_n]\n\ntarget = \"computer\"\nneighbors = get_nearest_neighbors(target, data_dict, top_n=6)\n\nprint(f\"Top 5 Nearest Neighbors for '{target}':\")\nfor word, score in neighbors:\n    print(f\"{word}: {score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:14:16.663182Z","iopub.execute_input":"2026-02-16T17:14:16.663385Z","iopub.status.idle":"2026-02-16T17:14:21.375570Z","shell.execute_reply.started":"2026-02-16T17:14:16.663326Z","shell.execute_reply":"2026-02-16T17:14:21.374846Z"}},"outputs":[{"name":"stdout","text":"Top 5 Nearest Neighbors for 'computer':\ncomputers: 0.9165\nsoftware: 0.8815\ntechnology: 0.8526\nelectronic: 0.8126\ninternet: 0.8060\ncomputing: 0.8026\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### Lexical Decision Activity","metadata":{}},{"cell_type":"code","source":"experiment_pairs = [\n    (\"butter\", \"bread\"),      # Related\n    (\"doctor\", \"nurse\"),      # Related/Unrelated (Distractor)\n    (\"tree\", \"doctor\"),       # Unrelated\n    (\"bread\", \"doctor\")       # Unrelated\n]\n\nprint(\"Cosine Similarity for Experiment Pairs\")\nfor w1, w2 in experiment_pairs:\n    try:\n        # Load words from data_dict\n        word1_vec = data_dict[w1]\n        word2_vec = data_dict[w2]\n        \n        # Calculate Cosine Similarity\n        cos_sim = cosine_similarity(word1_vec, word2_vec)\n        print(f\"Cosine similarity for pair ({w1}, {w2}) = {cos_sim:.4f}\")\n        \n    except KeyError as e:\n        print(f\"Word not found in dictionary: {e}\")\n\nprint(\"\\nEuclidean Distance for Experiment Pairs\")\nfor w1, w2 in experiment_pairs:\n    try:\n        # Load words from data_dict\n        word1_vec = data_dict[w1]\n        word2_vec = data_dict[w2]\n        \n        # Calculate Euclidean Distance\n        euc_dist = euclidean_distance(word1_vec, word2_vec)\n        print(f\"Euclidean distance for ({w1}, {w2}) = {euc_dist:.4f}\")\n        \n    except KeyError as e:\n        print(f\"Word not found in dictionary: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T17:14:21.376540Z","iopub.execute_input":"2026-02-16T17:14:21.376764Z","iopub.status.idle":"2026-02-16T17:14:21.384059Z","shell.execute_reply.started":"2026-02-16T17:14:21.376724Z","shell.execute_reply":"2026-02-16T17:14:21.383197Z"}},"outputs":[{"name":"stdout","text":"Cosine Similarity for Experiment Pairs\nCosine similarity for pair (butter, bread) = 0.8402\nCosine similarity for pair (doctor, nurse) = 0.7977\nCosine similarity for pair (tree, doctor) = 0.1691\nCosine similarity for pair (bread, doctor) = 0.1844\n\nEuclidean Distance for Experiment Pairs\nEuclidean distance for (butter, bread) = 3.3089\nEuclidean distance for (doctor, nurse) = 3.1275\nEuclidean distance for (tree, doctor) = 6.5470\nEuclidean distance for (bread, doctor) = 6.8686\n","output_type":"stream"}],"execution_count":23}]}