================================================================================
HYPOTHESIS TESTING
================================================================================

Loading data...
RT data: 848875 records
Frequency data: 32342 records
GPT-3 data: 12373 records

================================================================================
GPT-3 FILE STRUCTURE
================================================================================
Columns: ['token', 'logprob', 'offset', 'model', 'time', 'id', 'story']

First 5 rows:
      token    logprob  ...                                  id story
0        If        NaN  ...  cmpl-2XadldozPyY3qBl3GxvbDAX8XOE6V     0
1       you  -0.776271  ...  cmpl-2XadldozPyY3qBl3GxvbDAX8XOE6V     0
2      were  -4.331911  ...  cmpl-2XadldozPyY3qBl3GxvbDAX8XOE6V     0
3        to  -1.461822  ...  cmpl-2XadldozPyY3qBl3GxvbDAX8XOE6V     0
4   journey -10.098377  ...  cmpl-2XadldozPyY3qBl3GxvbDAX8XOE6V     0

[5 rows x 7 columns]

Data types:
token          str
logprob    float64
offset       int64
model          str
time         int64
id             str
story        int64
dtype: object

================================================================================
PREPARING RT DATA
================================================================================
Mean RT computed for 10256 unique word instances

================================================================================
MERGING WITH FREQUENCY DATA
================================================================================
After frequency merge: 32342 records

================================================================================
MERGING WITH GPT-3 PROBABILITIES
================================================================================

Identified columns:
  Story column: story
  Word column: token
  Probability column: logprob

Converted log probabilities to probabilities
